penalty(),
mixture(),
size = 20
)
# Tuning
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
# Finalize the model
best_params <- select_best(tune_results, "accuracy")
# Finalize the model
best_params <- select_best(tune_results, "accuracy")
print(tune_results)
# Tuning
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
print(tune_results)
# Finalize the model
best_params <- select_best(tune_results, "accuracy")
# Finalize the model
best_params <- select_best(tune_results, metric = "accuracy")
final_workflow <- finalize_workflow(creature_workflow, best_params)
# Train final model
final_fit <- fit(final_workflow, data = train_data)
# Predictions
predictions <- predict(final_fit, new_data = test_data)
# Submission file
submission <- tibble(
id = test_data$id,
type = predictions$.pred_class
)
# Tuning
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
print(tune_results)
# Finalize the model
best_params <- select_best(tune_results, metric = "accuracy")
final_workflow <- finalize_workflow(creature_workflow, best_params)
# Train final model
final_fit <- fit(final_workflow, data = train_data)
# Predictions
predictions <- predict(final_fit, new_data = test_data)
# Recipe
creature_recipe <-
recipe(type ~ ., data = train_data) %>%
step_string2factor(type) %>%
step_rm(id, color) %>%
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
step_normalize(all_numeric_predictors())
# Model specification (fixed typo here)
glmnet_spec <-
multinom_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# Predictions
predictions <- predict(final_fit, new_data = test_data)
# Recipe
creature_recipe <-
recipe(type ~ ., data = train_data) %>%
step_string2factor(type) %>%
step_rm(id, color) %>%
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
step_normalize(all_numeric_predictors())
# Model specification (fixed typo here)
glmnet_spec <-
multinom_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# Workflow
creature_workflow <-
workflow() %>%
add_recipe(creature_recipe) %>%
add_model(glmnet_spec)
# Cross-validation
cv_folds <- vfold_cv(train_data, v = 10, strata = type)
# Hyperparameter grid
tune_grid_values <- grid_latin_hypercube(
penalty(),
mixture(),
size = 20
)
# Tuning
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
# Finalize the model
best_params <- select_best(tune_results, metric = "accuracy")
final_workflow <- finalize_workflow(creature_workflow, best_params)
# Train final model
final_fit <- fit(final_workflow, data = train_data)
# Predictions
predictions <- predict(final_fit, new_data = test_data)
# -------------------------------------------------------------------------
# Kaggle - Ghouls, Ghosts, and Goblins Classifier
#
# This script uses the 'tidymodels' framework to build a multinomial
# regression model (using the 'glmnet' engine) to classify creatures.
#
# It includes:
# 1. Fast data loading with 'vroom'.
# 2. Feature engineering as requested.
# 3. Model tuning for 'penalty' and 'mixture' hyperparameters.
# 4. Prediction on the test set.
# 5. Generation of 'submission.csv'.
# -------------------------------------------------------------------------
# --- 1. Load Libraries ---
# Load the core tidymodels metapackage
library(tidymodels)
# Load parsnip explicitly to make sure multinomial_reg() is found
library(parsnip)
# Load vroom for fast CSV I/O
library(vroom)
# Optional: Load doParallel to speed up model tuning
# You'll need to install this package if you haven't:
# install.packages("doParallel")
library(doParallel)
# --- 2. Load Data ---
# Use vroom to read the training and test datasets
cat("Loading data...\n")
train_data <- vroom("./train.csv", show_col_types = FALSE)
test_data <- vroom("./test.csv", show_col_types = FALSE)
# --- 3. Set up Parallel Processing (Optional, but Recommended) ---
# Detect the number of physical cores and register a parallel backend
# This will significantly speed up the 'tune_grid' step.
cores <- parallel::detectCores(logical = FALSE)
cl <- makeCluster(cores)
registerDoParallel(cl)
cat(paste("Registered parallel backend with", cores, "cores.\n"))
# --- 4. Define the Recipe (Feature Engineering & Preprocessing) ---
# This is where we define all our preprocessing steps.
cat("Defining feature engineering recipe...\n")
creature_recipe <-
# Start with the training data, formula: predict 'type' from everything else
recipe(type ~ ., data = train_data) %>%
# Step 1: Convert the character outcome 'type' to a factor.
# This step is 'smart' and will be skipped when predicting on new
# data (like test_data) where 'type' doesn't exist.
step_string2factor(type) %>%
# Step 2: Remove variables we don't want
# 'id' is just an identifier
# 'color' is excluded as requested
step_rm(id, color) %>%
# Step 3: Perform the requested feature engineering (interactions)
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
# Step 4: Normalize all numeric predictors
# This is crucial for glmnet (penalized regression) to work well
step_normalize(all_numeric_predictors())
# You can check the recipe by "prepping" and "baking" it
# prep(creature_recipe) %>% bake(new_data = NULL) %>% print(n = 5)
# --- 5. Define the Model ---
# We want a multinomial regression model (3+ classes)
# We will use the 'glmnet' engine
# We set 'penalty' and 'mixture' to 'tune()' so we can find the best values
cat("Defining glmnet model specification...\n")
glmnet_spec <-
parsnip::multinom_reg( # Use parsnip:: to be explicit, and multinom_reg
penalty = tune(), # Main regularization parameter
mixture = tune()  # 0 = Ridge, 1 = Lasso, (0,1) = Elastic Net
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# --- 6. Create the Workflow ---
# A workflow bundles the recipe and the model together
cat("Creating workflow...\n")
creature_workflow <-
workflow() %>%
add_recipe(creature_recipe) %>%
add_model(glmnet_spec)
# --- 7. Tune the Model ---
# We need to find the best 'penalty' and 'mixture'
# We'll use 10-fold cross-validation
cat("Setting up 10-fold cross-validation...\n")
set.seed(123) # for reproducible folds
cv_folds <- vfold_cv(train_data, v = 10, strata = type)
# Define a grid of hyperparameters to try.
# We'll try 20 different combinations.
set.seed(456) # for reproducible grid
tune_grid_values <- grid_latin_hypercube(
penalty(),
mixture(),
size = 20
)
cat("Starting hyperparameter tuning (this may take a moment)...\n")
# Run the tuning process
# The 'control_grid' options are to save time and memory
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
# --- 8. Finalize the Model ---
# Find the single best set of hyperparameters based on 'accuracy'
cat("Selecting best model...\n")
best_params <- select_best(tune_results, metric = "accuracy")
# Finalize the workflow by updating it with these best parameters
final_workflow <- finalize_workflow(creature_workflow, best_params)
# --- 9. Train the Final Model ---
# Now, fit this one, best-performing model on the *entire* training dataset
cat("Training final model on all training data...\n")
final_fit <- fit(final_workflow, data = train_data)
# You can view the most important variables in the final model
# final_fit %>% extract_fit_parsnip() %>% vip()
# --- 10. Generate Predictions ---
cat("Generating predictions on test data...\n")
predictions <- predict(final_fit, new_data = test_data)
# -------------------------------------------------------------------------
# Kaggle - Ghouls, Ghosts, and Goblins Classifier
#
# This script uses the 'tidymodels' framework to build a multinomial
# regression model (using the 'glmnet' engine) to classify creatures.
#
# It includes:
# 1. Fast data loading with 'vroom'.
# 2. Feature engineering as requested.
# 3. Model tuning for 'penalty' and 'mixture' hyperparameters.
# 4. Prediction on the test set.
# 5. Generation of 'submission.csv'.
# -------------------------------------------------------------------------
# --- 1. Load Libraries ---
# Load the core tidymodels metapackage
library(tidymodels)
# Load parsnip explicitly to make sure multinomial_reg() is found
library(parsnip)
# Load vroom for fast CSV I/O
library(vroom)
# Optional: Load doParallel to speed up model tuning
# You'll need to install this package if you haven't:
# install.packages("doParallel")
library(doParallel)
# --- 2. Load Data ---
# Use vroom to read the training and test datasets
cat("Loading data...\n")
train_data <- vroom("./train.csv", show_col_types = FALSE)
test_data <- vroom("./test.csv", show_col_types = FALSE)
# --- 3. Set up Parallel Processing (Optional, but Recommended) ---
# Detect the number of physical cores and register a parallel backend
# This will significantly speed up the 'tune_grid' step.
cores <- parallel::detectCores(logical = FALSE)
cl <- makeCluster(cores)
registerDoParallel(cl)
cat(paste("Registered parallel backend with", cores, "cores.\n"))
# --- 4. Define the Recipe (Feature Engineering & Preprocessing) ---
# This is where we define all our preprocessing steps.
cat("Defining feature engineering recipe...\n")
creature_recipe <-
# Start with the training data, formula: predict 'type' from everything else
recipe(type ~ ., data = train_data) %>%
# Step 1: Convert the character outcome 'type' to a factor.
# This tells the recipe to find any/all variables with the
# role of "outcome" and convert them. It will be skipped
# automatically on new data that has no outcome column.
step_string2factor(all_outcomes()) %>%
# Step 2: Remove variables we don't want
# 'id' is just an identifier
# 'color' is excluded as requested
step_rm(id, color) %>%
# Step 3: Perform the requested feature engineering (interactions)
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
# Step 4: Normalize all numeric predictors
# This is crucial for glmnet (penalized regression) to work well
step_normalize(all_numeric_predictors())
# You can check the recipe by "prepping" and "baking" it
# prep(creature_recipe) %>% bake(new_data = NULL) %>% print(n = 5)
# --- 5. Define the Model ---
# We want a multinomial regression model (3+ classes)
# We will use the 'glmnet' engine
# We set 'penalty' and 'mixture' to 'tune()' so we can find the best values
cat("Defining glmnet model specification...\n")
glmnet_spec <-
parsnip::multinom_reg( # Use parsnip:: to be explicit, and multinom_reg
penalty = tune(), # Main regularization parameter
mixture = tune()  # 0 = Ridge, 1 = Lasso, (0,1) = Elastic Net
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# --- 6. Create the Workflow ---
# A workflow bundles the recipe and the model together
cat("Creating workflow...\n")
creature_workflow <-
workflow() %>%
add_recipe(creature_recipe) %>%
add_model(glmnet_spec)
# --- 7. Tune the Model ---
# We need to find the best 'penalty' and 'mixture'
# We'll use 10-fold cross-validation
cat("Setting up 10-fold cross-validation...\n")
set.seed(123) # for reproducible folds
cv_folds <- vfold_cv(train_data, v = 10, strata = type)
# Define a grid of hyperparameters to try.
# We'll try 20 different combinations.
set.seed(456) # for reproducible grid
tune_grid_values <- grid_latin_hypercube(
penalty(),
mixture(),
size = 20
)
cat("Starting hyperparameter tuning (this may take a moment)...\n")
# Run the tuning process
# The 'control_grid' options are to save time and memory
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
# --- 8. Finalize the Model ---
# Find the single best set of hyperparameters based on 'accuracy'
cat("Selecting best model...\n")
best_params <- select_best(tune_results, metric = "accuracy")
# Finalize the workflow by updating it with these best parameters
final_workflow <- finalize_workflow(creature_workflow, best_params)
# --- 9. Train the Final Model ---
# Now, fit this one, best-performing model on the *entire* training dataset
cat("Training final model on all training data...\n")
final_fit <- fit(final_workflow, data = train_data)
# You can view the most important variables in the final model
# final_fit %>% extract_fit_parsnip() %>% vip()
# --- 10. Generate Predictions ---
cat("Generating predictions on test data...\n")
predictions <- predict(final_fit, new_data = test_data)
# Libraries
library(tidymodels)
library(vroom)
library(parsnip)
# Load data
train_data <- vroom("./train.csv", show_col_types = FALSE)
test_data  <- vroom("./test.csv", show_col_types = FALSE)
# Recipe
creature_recipe <-
recipe(type ~ ., data = train_data) %>%
step_mutate(type = factor(type)) %>%
step_rm(id, color) %>%
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
step_normalize(all_numeric_predictors())
# Model specification (multinomial regression for multiclass classification)
glmnet_spec <-
multinom_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# Workflow
creature_workflow <-
workflow() %>%
add_recipe(creature_recipe) %>%
add_model(glmnet_spec)
# Cross-validation
cv_folds <- vfold_cv(train_data, v = 10, strata = type)
# Hyperparameter grid
tune_grid_values <- grid_latin_hypercube(
penalty(),
mixture(),
size = 20
)
# Tuning
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
# Finalize the model
best_params <- select_best(tune_results, metric = "accuracy")
final_workflow <- finalize_workflow(creature_workflow, best_params)
# Train final model
final_fit <- fit(final_workflow, data = train_data)
# Predictions
predictions <- predict(final_fit, new_data = test_data)
# Recipe
creature_recipe <-
recipe(type ~ ., data = train_data) %>%
step_rm(id, color) %>%
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
step_normalize(all_numeric_predictors())
# Libraries
library(tidymodels)
library(vroom)
library(parsnip)
# Load data
train_data <- vroom("./train.csv", show_col_types = FALSE)
test_data  <- vroom("./test.csv", show_col_types = FALSE)
# Recipe
creature_recipe <-
recipe(type ~ ., data = train_data) %>%
step_rm(id, color) %>%
step_mutate(
hair_soul  = hair_length * has_soul,
bone_flesh = bone_length * rotting_flesh,
bone_hair  = bone_length * hair_length,
bone_soul  = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul
) %>%
step_normalize(all_numeric_predictors())
# Model specification (multinomial regression for multiclass classification)
glmnet_spec <-
multinom_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# Workflow
creature_workflow <-
workflow() %>%
add_recipe(creature_recipe) %>%
add_model(glmnet_spec)
# Cross-validation
cv_folds <- vfold_cv(train_data, v = 10, strata = type)
# Hyperparameter grid
tune_grid_values <- grid_latin_hypercube(
penalty(),
mixture(),
size = 20
)
# Tuning
tune_results <- tune_grid(
creature_workflow,
resamples = cv_folds,
grid = tune_grid_values,
metrics = metric_set(accuracy),
control = control_grid(save_pred = FALSE, save_workflow = FALSE, verbose = TRUE)
)
# Finalize the model
best_params <- select_best(tune_results, metric = "accuracy")
final_workflow <- finalize_workflow(creature_workflow, best_params)
# Train final model
final_fit <- fit(final_workflow, data = train_data)
# Finalize the model
best_params <- select_best(tune_results, metric = "accuracy")
final_workflow <- finalize_workflow(creature_workflow, best_params)
# Train final model
final_fit <- fit(final_workflow, data = train_data)
# Predictions
predictions <- predict(final_fit, new_data = test_data)
# Submission file
submission <- tibble(
id = test_data$id,
type = predictions$.pred_class
)
vroom_write(submission, "submissionGLM.csv", delim = ",")
